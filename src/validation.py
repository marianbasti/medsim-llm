#!/usr/bin/env python
"""
Validation script for assessing the quality of patient responses generated by our model.
This is used to validate outputs both during training and inference.
"""

import json
import logging
import re
from typing import Dict, List, Optional, Union, Any, Tuple
import argparse
from pathlib import Path

import yaml
import torch
import numpy as np
from transformers import AutoModelForCausalLM, AutoTokenizer
import evaluate

logger = logging.getLogger(__name__)

# Add the missing function that synth_dialogues.py is trying to import
def validate_medical_content(dialogue_data: Dict) -> Tuple[bool, List[str]]:
    """
    Validates medical content in a dialogue for accuracy and consistency.
    
    Args:
        dialogue_data: Dictionary containing script and dialogue
        
    Returns:
        Tuple containing validation status (bool) and list of issues (if any)
    """
    issues = []
    script = dialogue_data.get("script", "")
    dialogue = dialogue_data.get("dialogue", [])
    
    # Check if dialogue exists and has turns
    if not dialogue:
        issues.append("Dialogue is empty")
        return False, issues
    
    # Ensure alternating doctor-patient turns
    last_role = None
    for i, turn in enumerate(dialogue):
        current_role = turn.get("role", "")
        
        # Skip first turn check
        if i > 0 and current_role == last_role:
            issues.append(f"Non-alternating turns detected at position {i}")
        
        last_role = current_role
    
    # Check for consistency with patient script
    script_str = json.dumps(script) if isinstance(script, dict) else str(script)
    dialogue_str = " ".join([turn.get("content", "") for turn in dialogue])
    
    # Extract key medical terms from script
    key_terms = set()
    
    # Look for medical conditions in script
    if isinstance(script, dict):
        # Extract medical conditions from structured script
        if "Medical History" in script and "conditions" in script["Medical History"]:
            key_terms.update(script["Medical History"]["conditions"])
        
        # Extract current symptoms
        if "Current Symptoms" in script:
            symptoms = script["Current Symptoms"]
            if isinstance(symptoms, dict):
                if "chief complaint" in symptoms:
                    key_terms.add(symptoms["chief complaint"])
                if "associated symptoms" in symptoms:
                    key_terms.update(re.findall(r'[\w\s]+(?:,|$)', symptoms["associated symptoms"]))
            elif isinstance(symptoms, str):
                key_terms.update(re.findall(r'[\w\s]+(?:,|$)', symptoms))
    else:
        # Try to extract from string script
        conditions_match = re.search(r'"conditions":\s*\[(.*?)\]', script_str)
        if conditions_match:
            conditions_str = conditions_match.group(1)
            key_terms.update(re.findall(r'"([^"]+)"', conditions_str))
        
        symptoms_match = re.search(r'"Current Symptoms".*?:.*?(?:"|\{)(.*?)(?:"|,\s*"|\})', script_str, re.DOTALL)
        if symptoms_match:
            symptoms_str = symptoms_match.group(1)
            key_terms.update(re.findall(r'(?:complaint|symptoms):\s*"([^"]+)"', symptoms_str))
    
    # Check if at least some key medical terms are mentioned in the dialogue
    mentioned_terms = 0
    for term in key_terms:
        if term.lower() in dialogue_str.lower():
            mentioned_terms += 1
    
    if mentioned_terms < min(2, len(key_terms)):
        issues.append("Dialogue doesn't sufficiently reference patient's medical conditions")
    
    # Check for dialogue length
    if len(dialogue) < 6:
        issues.append("Dialogue is too short (fewer than 6 turns)")
    
    # Return validation result
    return len(issues) == 0, issues


class PatientResponseValidator:
    """Validates patient responses for consistency, relevance, and accuracy."""
    
    def __init__(self, config_path: Optional[str] = None):
        """Initialize validator with optional custom config."""
        self.config = self._load_config(config_path)
        self.metrics = {
            "rouge": evaluate.load("rouge"),
            "bertscore": evaluate.load("bertscore"),
        }
        
        # Load sentence transformer for semantic similarity
        try:
            import sentence_transformers
            self.sentence_model = sentence_transformers.SentenceTransformer(
                "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
            )
        except ImportError:
            logger.warning("sentence-transformers not installed, semantic similarity will be limited")
            self.sentence_model = None
    
    def _load_config(self, config_path: Optional[str] = None) -> Dict:
        """Load configuration from file or use defaults."""
        if config_path:
            with open(config_path, 'r') as f:
                return yaml.safe_load(f)
        else:
            # Default configuration
            return {
                "validation": {
                    "metrics": ["consistency", "relevance", "script_adherence"],
                    "thresholds": {
                        "consistency": 0.7,
                        "relevance": 0.6,
                        "script_adherence": 0.5,
                        "overall": 0.65
                    },
                    "weights": {
                        "consistency": 0.4,
                        "relevance": 0.3,
                        "script_adherence": 0.3
                    }
                }
            }
    
    def calculate_consistency(self, dialogue_history: List[Dict], response: str) -> float:
        """
        Calculate consistency of response with previous dialogue history.
        
        Args:
            dialogue_history: List of previous dialogue turns
            response: Generated patient response
            
        Returns:
            Float score between 0 and 1
        """
        if not dialogue_history or len(dialogue_history) <= 2:
            return 1.0  # Not enough history to check consistency
        
        # Extract patient's previous responses
        patient_responses = [turn["content"] for turn in dialogue_history 
                           if turn.get("role") == "patient"]
        
        if not patient_responses:
            return 1.0  # No previous patient responses
        
        # Check for contradictions with previous responses
        contradiction_score = 1.0
        
        if self.sentence_model:
            # Use sentence transformers for semantic similarity
            embeddings = self.sentence_model.encode([response] + patient_responses)
            similarities = np.inner(embeddings[0], embeddings[1:])
            avg_similarity = np.mean(similarities)
            contradiction_score = min(1.0, max(0.0, avg_similarity))
        else:
            # Fallback to simple heuristic
            contradiction_score = 0.7  # Default reasonable score
        
        return contradiction_score
    
    def calculate_relevance(self, doctor_question: str, response: str) -> float:
        """
        Calculate relevance of response to doctor's question.
        
        Args:
            doctor_question: Doctor's question
            response: Generated patient response
            
        Returns:
            Float score between 0 and 1
        """
        # Calculate semantic similarity between question and response
        relevance_score = 0.7  # Default baseline score
        
        if self.sentence_model:
            embeddings = self.sentence_model.encode([doctor_question, response])
            similarity = np.inner(embeddings[0], embeddings[1])[0]
            relevance_score = min(1.0, max(0.0, similarity))
        
        return relevance_score
    
    def calculate_script_adherence(self, script: Dict, response: str) -> float:
        """
        Calculate adherence to patient script in response.
        
        Args:
            script: Patient script with demographics, medical history, etc.
            response: Generated patient response
            
        Returns:
            Float score between 0 and 1
        """
        # Default score
        adherence_score = 0.7
        
        # Convert script to string if it's a dictionary
        if isinstance(script, dict):
            script_str = json.dumps(script)
        else:
            script_str = str(script)
        
        # Calculate semantic similarity between script and response
        if self.sentence_model:
            embeddings = self.sentence_model.encode([script_str, response])
            similarity = np.inner(embeddings[0], embeddings[1])[0]
            adherence_score = min(1.0, max(0.0, similarity))
        
        return adherence_score
    
    def validate_response(
        self, 
        response: str, 
        doctor_question: str, 
        script: Dict, 
        dialogue_history: Optional[List[Dict]] = None
    ) -> Dict[str, Any]:
        """
        Validate a patient response.
        
        Args:
            response: Generated patient response
            doctor_question: Doctor's question
            script: Patient script
            dialogue_history: Previous dialogue turns
            
        Returns:
            Dictionary with scores and overall validation result
        """
        if dialogue_history is None:
            dialogue_history = []
        
        # Calculate individual scores
        consistency_score = self.calculate_consistency(dialogue_history, response)
        relevance_score = self.calculate_relevance(doctor_question, response)
        script_adherence_score = self.calculate_script_adherence(script, response)
        
        # Calculate weighted overall score
        weights = self.config["validation"]["weights"]
        overall_score = (
            weights["consistency"] * consistency_score +
            weights["relevance"] * relevance_score +
            weights["script_adherence"] * script_adherence_score
        )
        
        # Check if response passes thresholds
        thresholds = self.config["validation"]["thresholds"]
        passes_threshold = overall_score >= thresholds["overall"]
        
        return {
            "consistency_score": consistency_score,
            "relevance_score": relevance_score,
            "script_adherence_score": script_adherence_score,
            "overall_score": overall_score,
            "passes_validation": passes_threshold,
        }
    
    def validate_dataset(self, data_path: str, output_path: Optional[str] = None) -> Dict:
        """
        Validate a dataset of dialogues.
        
        Args:
            data_path: Path to dataset file (json or jsonl)
            output_path: Optional path to save validation results
            
        Returns:
            Dictionary with validation statistics
        """
        # Load dataset
        with open(data_path, 'r', encoding='utf-8') as f:
            if data_path.endswith('.jsonl'):
                data = [json.loads(line) for line in f]
            elif data_path.endswith('.json'):
                data = json.load(f)
            else:
                raise ValueError(f"Unsupported file format: {data_path}")
        
        # Results container
        results = []
        stats = {
            "total": 0,
            "passed": 0,
            "failed": 0,
            "avg_consistency": 0.0,
            "avg_relevance": 0.0,
            "avg_script_adherence": 0.0,
            "avg_overall": 0.0,
        }
        
        # Validate each dialogue
        for item in data:
            script = item["script"]
            dialogue = item["dialogue"]
            
            for i, turn in enumerate(dialogue):
                if turn["role"] == "patient":
                    # Get corresponding doctor question
                    if i > 0 and dialogue[i-1]["role"] == "doctor":
                        doctor_question = dialogue[i-1]["content"]
                    else:
                        doctor_question = ""  # No preceding doctor question
                    
                    # Get dialogue history
                    history = dialogue[:i]
                    
                    # Validate response
                    validation = self.validate_response(
                        response=turn["content"],
                        doctor_question=doctor_question,
                        script=script,
                        dialogue_history=history
                    )
                    
                    # Add to results
                    result = {
                        "patient_response": turn["content"],
                        "doctor_question": doctor_question,
                        "validation": validation
                    }
                    results.append(result)
                    
                    # Update statistics
                    stats["total"] += 1
                    if validation["passes_validation"]:
                        stats["passed"] += 1
                    else:
                        stats["failed"] += 1
                    
                    stats["avg_consistency"] += validation["consistency_score"]
                    stats["avg_relevance"] += validation["relevance_score"]
                    stats["avg_script_adherence"] += validation["script_adherence_score"]
                    stats["avg_overall"] += validation["overall_score"]
        
        # Calculate averages
        if stats["total"] > 0:
            stats["avg_consistency"] /= stats["total"]
            stats["avg_relevance"] /= stats["total"]
            stats["avg_script_adherence"] /= stats["total"]
            stats["avg_overall"] /= stats["total"]
            stats["pass_rate"] = stats["passed"] / stats["total"]
        
        # Save results if output path provided
        if output_path:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump({"results": results, "statistics": stats}, f, indent=2)
        
        return {"results": results, "statistics": stats}


def main():
    """Main function for command-line usage."""
    parser = argparse.ArgumentParser(description='Validate patient responses')
    parser.add_argument('--data_path', type=str, required=True, help='Path to data file (json or jsonl)')
    parser.add_argument('--output_path', type=str, help='Path to save validation results')
    parser.add_argument('--config_path', type=str, help='Path to config file')
    args = parser.parse_args()
    
    # Initialize validator
    validator = PatientResponseValidator(args.config_path)
    
    # Validate dataset
    results = validator.validate_dataset(args.data_path, args.output_path)
    
    # Print statistics
    stats = results["statistics"]
    print("Validation statistics:")
    print(f"Total responses: {stats['total']}")
    print(f"Passed: {stats['passed']} ({stats['pass_rate']:.2%})")
    print(f"Failed: {stats['failed']} ({1 - stats['pass_rate']:.2%})")
    print(f"Average consistency: {stats['avg_consistency']:.4f}")
    print(f"Average relevance: {stats['avg_relevance']:.4f}")
    print(f"Average script adherence: {stats['avg_script_adherence']:.4f}")
    print(f"Average overall score: {stats['avg_overall']:.4f}")


if __name__ == "__main__":
    main()