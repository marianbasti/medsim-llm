#!/usr/bin/env python
"""
Validation script for assessing the quality of patient responses generated by our model.
This is used to validate outputs both during training and inference.
"""

import json
import logging
import re
from typing import Dict, List, Optional, Union, Any, Tuple
import argparse
from pathlib import Path

import yaml
import torch
import numpy as np
from transformers import AutoModelForCausalLM, AutoTokenizer
import evaluate

# Configure logging with debug level
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',
    handlers=[
        logging.FileHandler('validation.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Add the missing function that synth_dialogues.py is trying to import
def validate_medical_content(dialogue_data: Dict) -> Tuple[bool, List[str]]:
    """
    Validates medical content in a dialogue for accuracy and consistency.
    
    Args:
        dialogue_data: Dictionary containing script and dialogue
        
    Returns:
        Tuple containing validation status (bool) and list of issues (if any)
    """
    issues = []
    script = dialogue_data.get("script", "")
    dialogue = dialogue_data.get("dialogue", [])
    
    logger.debug(f"Validating dialogue with {len(dialogue)} turns")
    
    # Check if dialogue exists and has turns
    if not dialogue:
        issues.append("Dialogue is empty")
        return False, issues
    
    # Ensure alternating doctor-patient turns
    last_role = None
    for i, turn in enumerate(dialogue):
        current_role = turn.get("role", "")
        
        # Skip first turn check
        if i > 0 and current_role == last_role:
            logger.debug(f"Non-alternating turn detected at position {i}: {current_role}")
            issues.append(f"Non-alternating turns detected at position {i}")
        
        last_role = current_role
    
    # Check for consistency with patient script
    script_str = json.dumps(script) if isinstance(script, dict) else str(script)
    dialogue_str = " ".join([turn.get("content", "") for turn in dialogue])
    
    # Extract key medical terms from script
    key_terms = set()
    
    # Look for medical conditions in script
    if isinstance(script, dict):
        # Extract medical conditions from structured script
        logger.debug("Extracting medical terms from structured script")
        if "Medical History" in script and "conditions" in script["Medical History"]:
            key_terms.update(script["Medical History"]["conditions"])
            logger.debug(f"Found conditions: {script['Medical History']['conditions']}")
        
        # Extract current symptoms
        if "Current Symptoms" in script:
            symptoms = script["Current Symptoms"]
            if isinstance(symptoms, dict):
                if "chief complaint" in symptoms:
                    key_terms.add(symptoms["chief complaint"])
                if "associated symptoms" in symptoms:
                    key_terms.update(re.findall(r'[\w\s]+(?:,|$)', symptoms["associated symptoms"]))
            elif isinstance(symptoms, str):
                key_terms.update(re.findall(r'[\w\s]+(?:,|$)', symptoms))
            
            logger.debug(f"Extracted symptom terms: {key_terms}")
    else:
        # Try to extract from string script
        logger.debug("Attempting to extract medical terms from script string")
        conditions_match = re.search(r'"conditions":\s*\[(.*?)\]', script_str)
        if conditions_match:
            conditions_str = conditions_match.group(1)
            key_terms.update(re.findall(r'"([^"]+)"', conditions_str))
        
        symptoms_match = re.search(r'"Current Symptoms".*?:.*?(?:"|\{)(.*?)(?:"|,\s*"|\})', script_str, re.DOTALL)
        if symptoms_match:
            symptoms_str = symptoms_match.group(1)
            key_terms.update(re.findall(r'(?:complaint|symptoms):\s*"([^"]+)"', symptoms_str))
    
    # Check if at least some key medical terms are mentioned in the dialogue
    mentioned_terms = 0
    for term in key_terms:
        if term.lower() in dialogue_str.lower():
            mentioned_terms += 1
            logger.debug(f"Found term '{term}' in dialogue")
    
    if mentioned_terms < min(2, len(key_terms)):
        logger.warning(f"Dialogue only mentions {mentioned_terms} out of {len(key_terms)} medical terms")
        issues.append("Dialogue doesn't sufficiently reference patient's medical conditions")
    
    # Check for dialogue length
    if len(dialogue) < 6:
        logger.debug(f"Dialogue too short: {len(dialogue)} turns")
        issues.append("Dialogue is too short (fewer than 6 turns)")
    
    # Return validation result
    is_valid = len(issues) == 0
    logger.debug(f"Validation result: {is_valid}, issues: {len(issues)}")
    return is_valid, issues


class PatientResponseValidator:
    """Validates patient responses for consistency, relevance, and accuracy."""
    
    def __init__(self, config_path: Optional[str] = None):
        """Initialize validator with optional custom config."""
        logger.debug(f"Initializing PatientResponseValidator with config path: {config_path}")
        self.config = self._load_config(config_path)
        logger.debug("Loading evaluation metrics")
        self.metrics = {
            "rouge": evaluate.load("rouge"),
            "bertscore": evaluate.load("bertscore"),
        }
        
        # Load sentence transformer for semantic similarity
        try:
            import sentence_transformers
            logger.debug("Loading sentence transformer model")
            self.sentence_model = sentence_transformers.SentenceTransformer(
                "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
            )
            logger.debug("Sentence transformer model loaded successfully")
        except ImportError:
            logger.warning("sentence-transformers not installed, semantic similarity will be limited")
            self.sentence_model = None
    
    def _load_config(self, config_path: Optional[str] = None) -> Dict:
        """Load configuration from file or use defaults."""
        if config_path:
            logger.debug(f"Loading config from file: {config_path}")
            with open(config_path, 'r') as f:
                return yaml.safe_load(f)
        else:
            logger.debug("Using default configuration")
            # Default configuration
            return {
                "validation": {
                    "metrics": ["consistency", "relevance", "script_adherence"],
                    "thresholds": {
                        "consistency": 0.7,
                        "relevance": 0.6,
                        "script_adherence": 0.5,
                        "overall": 0.65
                    },
                    "weights": {
                        "consistency": 0.4,
                        "relevance": 0.3,
                        "script_adherence": 0.3
                    }
                }
            }
    
    def calculate_consistency(self, dialogue_history: List[Dict], response: str) -> float:
        """
        Calculate consistency of response with previous dialogue history.
        
        Args:
            dialogue_history: List of previous dialogue turns
            response: Generated patient response
            
        Returns:
            Float score between 0 and 1
        """
        if not dialogue_history or len(dialogue_history) <= 2:
            logger.debug("Not enough dialogue history to calculate consistency")
            return 1.0  # Not enough history to check consistency
        
        # Extract patient's previous responses
        patient_responses = [turn["content"] for turn in dialogue_history 
                           if turn.get("role") == "patient"]
        
        if not patient_responses:
            logger.debug("No previous patient responses found")
            return 1.0  # No previous patient responses
        
        # Check for contradictions with previous responses
        contradiction_score = 1.0
        
        if self.sentence_model:
            logger.debug(f"Calculating semantic similarity with {len(patient_responses)} previous responses")
            # Use sentence transformers for semantic similarity
            embeddings = self.sentence_model.encode([response] + patient_responses)
            similarities = np.inner(embeddings[0], embeddings[1:])
            avg_similarity = np.mean(similarities)
            contradiction_score = min(1.0, max(0.0, avg_similarity))
            logger.debug(f"Calculated consistency score: {contradiction_score}")
        else:
            # Fallback to simple heuristic
            logger.debug("Using fallback heuristic for consistency score")
            contradiction_score = 0.7  # Default reasonable score
        
        return contradiction_score
    
    def calculate_relevance(self, doctor_question: str, response: str) -> float:
        """
        Calculate relevance of response to doctor's question.
        
        Args:
            doctor_question: Doctor's question
            response: Generated patient response
            
        Returns:
            Float score between 0 and 1
        """
        # Calculate semantic similarity between question and response
        relevance_score = 0.7  # Default baseline score
        
        if self.sentence_model:
            embeddings = self.sentence_model.encode([doctor_question, response])
            similarity = np.inner(embeddings[0], embeddings[1])[0]
            relevance_score = min(1.0, max(0.0, similarity))
        
        return relevance_score
    
    def calculate_script_adherence(self, script: Dict, response: str) -> float:
        """
        Calculate adherence to patient script in response.
        
        Args:
            script: Patient script with demographics, medical history, etc.
            response: Generated patient response
            
        Returns:
            Float score between 0 and 1
        """
        # Default score
        adherence_score = 0.7
        
        # Convert script to string if it's a dictionary
        if isinstance(script, dict):
            script_str = json.dumps(script)
        else:
            script_str = str(script)
        
        # Calculate semantic similarity between script and response
        if self.sentence_model:
            embeddings = self.sentence_model.encode([script_str, response])
            similarity = np.inner(embeddings[0], embeddings[1])[0]
            adherence_score = min(1.0, max(0.0, similarity))
        
        return adherence_score
    
    def validate_response(
        self, 
        response: str, 
        doctor_question: str, 
        script: Dict, 
        dialogue_history: Optional[List[Dict]] = None
    ) -> Dict[str, Any]:
        """
        Validate a patient response.
        
        Args:
            response: Generated patient response
            doctor_question: Doctor's question
            script: Patient script
            dialogue_history: Previous dialogue turns
            
        Returns:
            Dictionary with scores and overall validation result
        """
        if dialogue_history is None:
            dialogue_history = []
        
        # Calculate individual scores
        consistency_score = self.calculate_consistency(dialogue_history, response)
        relevance_score = self.calculate_relevance(doctor_question, response)
        script_adherence_score = self.calculate_script_adherence(script, response)
        
        # Calculate weighted overall score
        weights = self.config["validation"]["weights"]
        overall_score = (
            weights["consistency"] * consistency_score +
            weights["relevance"] * relevance_score +
            weights["script_adherence"] * script_adherence_score
        )
        
        # Check if response passes thresholds
        thresholds = self.config["validation"]["thresholds"]
        passes_threshold = overall_score >= thresholds["overall"]
        
        return {
            "consistency_score": consistency_score,
            "relevance_score": relevance_score,
            "script_adherence_score": script_adherence_score,
            "overall_score": overall_score,
            "passes_validation": passes_threshold,
        }
    
    def validate_dataset(self, data_path: str, output_path: Optional[str] = None) -> Dict:
        """
        Validate a dataset of dialogues.
        
        Args:
            data_path: Path to dataset file (json or jsonl)
            output_path: Optional path to save validation results
            
        Returns:
            Dictionary with validation statistics
        """
        logger.debug(f"Validating dataset: {data_path}")
        
        # Load dataset
        with open(data_path, 'r', encoding='utf-8') as f:
            if data_path.endswith('.jsonl'):
                data = [json.loads(line) for line in f]
                logger.debug(f"Loaded {len(data)} items from JSONL file")
            elif data_path.endswith('.json'):
                data = json.load(f)
                logger.debug(f"Loaded data from JSON file")
            else:
                error_msg = f"Unsupported file format: {data_path}"
                logger.error(error_msg)
                raise ValueError(error_msg)
        
        # Results container
        results = []
        stats = {
            "total": 0,
            "passed": 0,
            "failed": 0,
            "avg_consistency": 0.0,
            "avg_relevance": 0.0,
            "avg_script_adherence": 0.0,
            "avg_overall": 0.0,
        }
        
        logger.info(f"Starting validation of {len(data)} dialogues")
        
        # Validate each dialogue
        for item_idx, item in enumerate(data):
            logger.debug(f"Validating dialogue {item_idx+1}/{len(data)}")
            script = item["script"]
            dialogue = item["dialogue"]
            
            for i, turn in enumerate(dialogue):
                if turn["role"] == "patient":
                    # Get corresponding doctor question
                    if i > 0 and dialogue[i-1]["role"] == "doctor":
                        doctor_question = dialogue[i-1]["content"]
                    else:
                        doctor_question = ""  # No preceding doctor question
                    
                    # Get dialogue history
                    history = dialogue[:i]
                    
                    # Validate response
                    validation = self.validate_response(
                        response=turn["content"],
                        doctor_question=doctor_question,
                        script=script,
                        dialogue_history=history
                    )
                    
                    # Add to results
                    result = {
                        "patient_response": turn["content"],
                        "doctor_question": doctor_question,
                        "validation": validation
                    }
                    results.append(result)
                    
                    # Update statistics
                    stats["total"] += 1
                    if validation["passes_validation"]:
                        stats["passed"] += 1
                    else:
                        stats["failed"] += 1
                    
                    stats["avg_consistency"] += validation["consistency_score"]
                    stats["avg_relevance"] += validation["relevance_score"]
                    stats["avg_script_adherence"] += validation["script_adherence_score"]
                    stats["avg_overall"] += validation["overall_score"]
        
        # Calculate averages
        if stats["total"] > 0:
            stats["avg_consistency"] /= stats["total"]
            stats["avg_relevance"] /= stats["total"]
            stats["avg_script_adherence"] /= stats["total"]
            stats["avg_overall"] /= stats["total"]
            stats["pass_rate"] = stats["passed"] / stats["total"]
            logger.info(f"Validation complete - Pass rate: {stats['pass_rate']:.2%}")
        
        # Save results if output path provided
        if output_path:
            logger.debug(f"Saving validation results to {output_path}")
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump({"results": results, "statistics": stats}, f, indent=2)
        
        return {"results": results, "statistics": stats}


def main():
    """Main function for command-line usage."""
    parser = argparse.ArgumentParser(description='Validate patient responses')
    parser.add_argument('--data_path', type=str, required=True, help='Path to data file (json or jsonl)')
    parser.add_argument('--output_path', type=str, help='Path to save validation results')
    parser.add_argument('--config_path', type=str, help='Path to config file')
    parser.add_argument('--log_level', type=str, default='INFO',
                        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'],
                        help='Set the logging level')
    args = parser.parse_args()
    
    # Set logging level from command line
    logging.getLogger().setLevel(getattr(logging, args.log_level))
    logger.debug(f"Logging level set to {args.log_level}")
    
    # Initialize validator
    validator = PatientResponseValidator(args.config_path)
    
    # Validate dataset
    results = validator.validate_dataset(args.data_path, args.output_path)
    
    # Print statistics
    stats = results["statistics"]
    print("Validation statistics:")
    print(f"Total responses: {stats['total']}")
    print(f"Passed: {stats['passed']} ({stats['pass_rate']:.2%})")
    print(f"Failed: {stats['failed']} ({1 - stats['pass_rate']:.2%})")
    print(f"Average consistency: {stats['avg_consistency']:.4f}")
    print(f"Average relevance: {stats['avg_relevance']:.4f}")
    print(f"Average script adherence: {stats['avg_script_adherence']:.4f}")
    print(f"Average overall score: {stats['avg_overall']:.4f}")


if __name__ == "__main__":
    main()