generation:
  batch_size: 1
  num_workers: 1
  validation:
    enabled: true
    max_turns: 75
    min_turns: 8
llm:
  api_key: citecca
  base_url: http://localhost:7000/v1/
  max_retries: 3
  temperature: 1.0
  timeout: 70
output:
  backup_enabled: true
  format: jsonl
  path: data/generated

# Fine-tuning parameters
fine_tuning:
  model:
    name: "meta-llama/Llama-3.2-1B"
    use_4bit: false
    use_flash_attention: true
  training:
    batch_size: 1
    gradient_accumulation_steps: 2
    learning_rate: 2.0e-5
    weight_decay: 0.01
    warmup_ratio: 0.03
    eval_steps: 10
    save_steps: 150
    max_seq_length: 2048
    lora_alpha: 32
    lora_dropout: 0.1
    lora_r: 64
    epochs: 10
  data:
    train_file: "dialogo_medico-paciente_es.jsonl"
    validation_file: null
    validation_split: 0.1
  output:
    dir: "models/patient-Llama-3.2-1B"
    checkpoint_dir: "checkpoints"
  validation:
    metrics: ["consistency", "relevance", "script_adherence"]
    thresholds:
      consistency: 0.7
      relevance: 0.6
      script_adherence: 0.5
      overall: 0.65
    weights:
      consistency: 0.4
      relevance: 0.3
      script_adherence: 0.3
